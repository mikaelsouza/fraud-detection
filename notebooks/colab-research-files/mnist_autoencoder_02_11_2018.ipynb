{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mnist-autoencoder-02-11-2018.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mikaelsouza/fraud-detection/blob/master/notebooks/colab-research-files/mnist_autoencoder_02_11_2018.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "plPByAJUYPtG",
        "colab_type": "code",
        "outputId": "94c4781e-b2a2-4faa-c286-8a01962d874c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import keras\n",
        "import numpy as np\n",
        "from sklearn import model_selection, preprocessing, metrics, utils\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.layers.advanced_activations import LeakyReLU"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "XrzeEHWzpCgy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Loading dataset\n",
        "(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()\n",
        "X_train = X_train / 255\n",
        "X_train = X_train.reshape(-1, 784)\n",
        "X_test = X_test.reshape(-1, 784)\n",
        "X_test = X_test / 255\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Gx4pv_3sp6lK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Filtering dataset to use only 2 classes\n",
        "\n",
        "def get_labeled_instances(X_data, y_data, wanted_label):\n",
        "  filtered_data = []\n",
        "  for data, label in zip(X_data, y_data):\n",
        "    if label == wanted_label:\n",
        "      filtered_data.append((data, label))\n",
        "  new_X, new_y = zip(*filtered_data)\n",
        "  new_X = np.array(new_X)\n",
        "  new_y = np.array(new_y)\n",
        "  return new_X, new_y\n",
        "\n",
        "class_0, label_0 = get_labeled_instances(X_train, y_train, 0)\n",
        "class_1, label_1 = get_labeled_instances(X_train, y_train, 1)\n",
        "\n",
        "test_class_0, test_label_0 = get_labeled_instances(X_test, y_test, 0)\n",
        "test_class_1, test_label_1 = get_labeled_instances(X_test, y_test, 1)\n",
        "\n",
        "label_0 = label_0.reshape(-1, 1)\n",
        "label_1 = label_1.reshape(-1, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0NVGAv3nzYpB",
        "colab_type": "code",
        "outputId": "7a163793-3204-43c9-b3a6-3020fcbdf706",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "# Generating a imbalanced dataframe\n",
        "\n",
        "imbalanced_class_0 = class_0[:200]\n",
        "imbalanced_label_0 = label_0[:200]\n",
        "\n",
        "imbalanced_data_0 = np.concatenate((imbalanced_class_0,\n",
        "                                    imbalanced_label_0),\n",
        "                                    axis=1)\n",
        "\n",
        "imbalanced_data_1 = np.concatenate((class_1, label_1), axis=1)\n",
        "\n",
        "print(imbalanced_data_0.shape)\n",
        "print(imbalanced_data_1.shape)\n",
        "\n",
        "columns = [*list(range(784)), 'class']\n",
        "\n",
        "imbalanced_dataframe = pd.DataFrame(np.concatenate((imbalanced_data_0,\n",
        "                                                    imbalanced_data_1)),\n",
        "                                    columns=columns)\n",
        "\n",
        "imbalanced_dataframe.head()\n",
        "\n",
        "# Defining test dataset using 0s and 1s.\n",
        "\n",
        "X_test = np.concatenate((test_class_0,\n",
        "                         test_class_1,),\n",
        "                        axis=0)\n",
        "\n",
        "y_test = np.concatenate((test_label_0,\n",
        "                         test_label_1,),\n",
        "                        axis=0)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(200, 785)\n",
            "(6742, 785)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "LDIl9F1O0lqM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Function to get stratified samples\n",
        "# https://stackoverflow.com/questions/44114463/stratified-sampling-in-pandas#\n",
        "def stratified_sample(dataframe, feature, size):\n",
        "  return dataframe.groupby(feature, group_keys=False).apply(lambda x: x.sample(min(len(x), size)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KBTWF4Szyaje",
        "colab_type": "code",
        "outputId": "df2a3a0d-ff68-4db2-82bf-df984b1e1ef0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        }
      },
      "cell_type": "code",
      "source": [
        "# Defining other necessary variables\n",
        "\n",
        "epochs = 400\n",
        "batch_size = 256\n",
        "\n",
        "X_train, y_train = imbalanced_dataframe.drop(['class'], axis=1), imbalanced_dataframe['class']\n",
        "X_train = np.array(X_train).reshape(-1, 784)\n",
        "y_train = y_train.reshape(-1, 1)\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
            "  import sys\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "Gw4WViMp6Afa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def plot_mnist_value(sample):\n",
        "  sample = sample * 255\n",
        "  pixels = sample\n",
        "  pixels = pixels.reshape((28, 28))\n",
        "  plt.imshow(pixels, cmap='gray')\n",
        "  plt.show()\n",
        "  \n",
        "\n",
        "def sample_images(predictions, labels):\n",
        "  r, c = 2, 5\n",
        "  gen_imgs = predictions\n",
        "  gen_imgs = gen_imgs.reshape(-1, 28, 28)\n",
        "  \n",
        "  fig, axs = plt.subplots(r, c)\n",
        "  for j in range(c):\n",
        "      axs[0,j].imshow(gen_imgs[j], cmap='gray')\n",
        "      axs[0,j].set_title(\"Digit: %d\" % labels[j])\n",
        "      axs[0,j].axis('off')\n",
        "  for j in range(-1, - c - 1, -1):\n",
        "      axs[1,j].imshow(gen_imgs[j], cmap='gray')\n",
        "      axs[1,j].set_title(\"Digit: %d\" % labels[j])\n",
        "      axs[1,j].axis('off')\n",
        "  plt.show()\n",
        "  plt.close()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Hxe0UYrs9iz_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Defining autoencoder\n",
        "\n",
        "encoder = keras.models.Sequential()\n",
        "encoder.add(Dense(1024, input_dim=784))\n",
        "encoder.add(LeakyReLU(0.2))\n",
        "encoder.add(Dropout(0.6))\n",
        "encoder.add(Dense(256))\n",
        "encoder.add(LeakyReLU(0.2))\n",
        "encoder.add(Dropout(0.6))\n",
        "\n",
        "# Defining encoded layer\n",
        "\n",
        "code = keras.layers.Dense(128)\n",
        "\n",
        "# Defining decoder\n",
        "\n",
        "decoder = keras.models.Sequential()\n",
        "decoder.add(Dense(256, input_shape=(128,)))\n",
        "decoder.add(LeakyReLU(0.2))\n",
        "decoder.add(Dropout(0.6))\n",
        "decoder.add(Dense(1024))\n",
        "decoder.add(LeakyReLU(0.2))\n",
        "decoder.add(Dropout(0.6))\n",
        "decoder.add(Dense(784, activation='sigmoid'))\n",
        "\n",
        "encoder_input = keras.layers.Input(shape=(784,))\n",
        "\n",
        "ae_model = decoder(code(encoder(encoder_input)))\n",
        "autoencoder = keras.models.Model(encoder_input, ae_model)\n",
        "\n",
        "adam = keras.optimizers.Adam(lr=0.0002, beta_1=0.5)\n",
        "\n",
        "autoencoder.compile(optimizer='adam', loss='mse')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PaZ9EUb5cbb8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 13634
        },
        "outputId": "98aaba9a-d420-42f7-8703-03245700a968"
      },
      "cell_type": "code",
      "source": [
        "history = autoencoder.fit(X_train, X_train,\n",
        "                epochs=epochs,\n",
        "                batch_size=batch_size,\n",
        "                validation_data=(X_test, X_test))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 6942 samples, validate on 2115 samples\n",
            "Epoch 1/400\n",
            "6942/6942 [==============================] - 4s 648us/step - loss: 0.0768 - val_loss: 0.1061\n",
            "Epoch 2/400\n",
            "6942/6942 [==============================] - 4s 536us/step - loss: 0.0494 - val_loss: 0.1031\n",
            "Epoch 3/400\n",
            "6942/6942 [==============================] - 4s 522us/step - loss: 0.0403 - val_loss: 0.0786\n",
            "Epoch 4/400\n",
            "6942/6942 [==============================] - 4s 514us/step - loss: 0.0267 - val_loss: 0.0590\n",
            "Epoch 5/400\n",
            "6942/6942 [==============================] - 4s 520us/step - loss: 0.0198 - val_loss: 0.0448\n",
            "Epoch 6/400\n",
            "6942/6942 [==============================] - 3s 501us/step - loss: 0.0173 - val_loss: 0.0423\n",
            "Epoch 7/400\n",
            "6942/6942 [==============================] - 3s 497us/step - loss: 0.0153 - val_loss: 0.0375\n",
            "Epoch 8/400\n",
            "6942/6942 [==============================] - 3s 503us/step - loss: 0.0139 - val_loss: 0.0370\n",
            "Epoch 9/400\n",
            "6942/6942 [==============================] - 4s 507us/step - loss: 0.0125 - val_loss: 0.0325\n",
            "Epoch 10/400\n",
            "6942/6942 [==============================] - 4s 511us/step - loss: 0.0117 - val_loss: 0.0344\n",
            "Epoch 11/400\n",
            "6942/6942 [==============================] - 4s 507us/step - loss: 0.0112 - val_loss: 0.0299\n",
            "Epoch 12/400\n",
            "6942/6942 [==============================] - 3s 504us/step - loss: 0.0107 - val_loss: 0.0298\n",
            "Epoch 13/400\n",
            "6942/6942 [==============================] - 4s 521us/step - loss: 0.0104 - val_loss: 0.0289\n",
            "Epoch 14/400\n",
            "6942/6942 [==============================] - 4s 523us/step - loss: 0.0100 - val_loss: 0.0279\n",
            "Epoch 15/400\n",
            "6942/6942 [==============================] - 4s 518us/step - loss: 0.0098 - val_loss: 0.0272\n",
            "Epoch 16/400\n",
            "6942/6942 [==============================] - 4s 519us/step - loss: 0.0095 - val_loss: 0.0278\n",
            "Epoch 17/400\n",
            "6942/6942 [==============================] - 4s 520us/step - loss: 0.0093 - val_loss: 0.0258\n",
            "Epoch 18/400\n",
            "6942/6942 [==============================] - 4s 516us/step - loss: 0.0091 - val_loss: 0.0254\n",
            "Epoch 19/400\n",
            "6942/6942 [==============================] - 4s 517us/step - loss: 0.0090 - val_loss: 0.0250\n",
            "Epoch 20/400\n",
            "6942/6942 [==============================] - 4s 521us/step - loss: 0.0087 - val_loss: 0.0250\n",
            "Epoch 21/400\n",
            "6942/6942 [==============================] - 4s 517us/step - loss: 0.0087 - val_loss: 0.0266\n",
            "Epoch 22/400\n",
            "6942/6942 [==============================] - 4s 538us/step - loss: 0.0085 - val_loss: 0.0246\n",
            "Epoch 23/400\n",
            "6942/6942 [==============================] - 4s 523us/step - loss: 0.0084 - val_loss: 0.0257\n",
            "Epoch 24/400\n",
            "6942/6942 [==============================] - 4s 514us/step - loss: 0.0083 - val_loss: 0.0237\n",
            "Epoch 25/400\n",
            "6942/6942 [==============================] - 4s 507us/step - loss: 0.0082 - val_loss: 0.0226\n",
            "Epoch 26/400\n",
            "6942/6942 [==============================] - 4s 509us/step - loss: 0.0083 - val_loss: 0.0244\n",
            "Epoch 27/400\n",
            "6942/6942 [==============================] - 4s 518us/step - loss: 0.0082 - val_loss: 0.0235\n",
            "Epoch 28/400\n",
            "6942/6942 [==============================] - 4s 504us/step - loss: 0.0081 - val_loss: 0.0228\n",
            "Epoch 29/400\n",
            "6942/6942 [==============================] - 4s 530us/step - loss: 0.0079 - val_loss: 0.0226\n",
            "Epoch 30/400\n",
            "6942/6942 [==============================] - 4s 524us/step - loss: 0.0079 - val_loss: 0.0223\n",
            "Epoch 31/400\n",
            "6942/6942 [==============================] - 4s 524us/step - loss: 0.0078 - val_loss: 0.0218\n",
            "Epoch 32/400\n",
            "6942/6942 [==============================] - 4s 516us/step - loss: 0.0078 - val_loss: 0.0212\n",
            "Epoch 33/400\n",
            "6942/6942 [==============================] - 4s 508us/step - loss: 0.0078 - val_loss: 0.0204\n",
            "Epoch 34/400\n",
            "6942/6942 [==============================] - 4s 511us/step - loss: 0.0078 - val_loss: 0.0214\n",
            "Epoch 35/400\n",
            "6942/6942 [==============================] - 4s 517us/step - loss: 0.0077 - val_loss: 0.0201\n",
            "Epoch 36/400\n",
            "6942/6942 [==============================] - 4s 523us/step - loss: 0.0076 - val_loss: 0.0228\n",
            "Epoch 37/400\n",
            "6942/6942 [==============================] - 4s 516us/step - loss: 0.0076 - val_loss: 0.0204\n",
            "Epoch 38/400\n",
            "6942/6942 [==============================] - 4s 521us/step - loss: 0.0075 - val_loss: 0.0212\n",
            "Epoch 39/400\n",
            "6942/6942 [==============================] - 4s 521us/step - loss: 0.0075 - val_loss: 0.0202\n",
            "Epoch 40/400\n",
            "6942/6942 [==============================] - 4s 517us/step - loss: 0.0075 - val_loss: 0.0204\n",
            "Epoch 41/400\n",
            "6942/6942 [==============================] - 3s 504us/step - loss: 0.0074 - val_loss: 0.0207\n",
            "Epoch 42/400\n",
            "6942/6942 [==============================] - 3s 500us/step - loss: 0.0075 - val_loss: 0.0193\n",
            "Epoch 43/400\n",
            "6942/6942 [==============================] - 4s 504us/step - loss: 0.0074 - val_loss: 0.0194\n",
            "Epoch 44/400\n",
            "6942/6942 [==============================] - 3s 501us/step - loss: 0.0074 - val_loss: 0.0196\n",
            "Epoch 45/400\n",
            "6942/6942 [==============================] - 4s 506us/step - loss: 0.0074 - val_loss: 0.0215\n",
            "Epoch 46/400\n",
            "6942/6942 [==============================] - 3s 497us/step - loss: 0.0074 - val_loss: 0.0202\n",
            "Epoch 47/400\n",
            "6942/6942 [==============================] - 3s 491us/step - loss: 0.0073 - val_loss: 0.0194\n",
            "Epoch 48/400\n",
            "6942/6942 [==============================] - 3s 496us/step - loss: 0.0073 - val_loss: 0.0202\n",
            "Epoch 49/400\n",
            "6942/6942 [==============================] - 4s 505us/step - loss: 0.0073 - val_loss: 0.0205\n",
            "Epoch 50/400\n",
            "6942/6942 [==============================] - 3s 500us/step - loss: 0.0074 - val_loss: 0.0196\n",
            "Epoch 51/400\n",
            "6942/6942 [==============================] - 3s 497us/step - loss: 0.0072 - val_loss: 0.0206\n",
            "Epoch 52/400\n",
            "6942/6942 [==============================] - 3s 495us/step - loss: 0.0073 - val_loss: 0.0194\n",
            "Epoch 53/400\n",
            "6942/6942 [==============================] - 3s 498us/step - loss: 0.0073 - val_loss: 0.0182\n",
            "Epoch 54/400\n",
            "6942/6942 [==============================] - 3s 499us/step - loss: 0.0071 - val_loss: 0.0191\n",
            "Epoch 55/400\n",
            "6942/6942 [==============================] - 4s 521us/step - loss: 0.0072 - val_loss: 0.0195\n",
            "Epoch 56/400\n",
            "6942/6942 [==============================] - 4s 546us/step - loss: 0.0071 - val_loss: 0.0186\n",
            "Epoch 57/400\n",
            "6942/6942 [==============================] - 4s 549us/step - loss: 0.0071 - val_loss: 0.0182\n",
            "Epoch 58/400\n",
            "6942/6942 [==============================] - 4s 540us/step - loss: 0.0071 - val_loss: 0.0197\n",
            "Epoch 59/400\n",
            "6942/6942 [==============================] - 4s 545us/step - loss: 0.0070 - val_loss: 0.0190\n",
            "Epoch 60/400\n",
            "6942/6942 [==============================] - 3s 500us/step - loss: 0.0071 - val_loss: 0.0182\n",
            "Epoch 61/400\n",
            "6942/6942 [==============================] - 3s 502us/step - loss: 0.0071 - val_loss: 0.0191\n",
            "Epoch 62/400\n",
            "6942/6942 [==============================] - 3s 496us/step - loss: 0.0071 - val_loss: 0.0185\n",
            "Epoch 63/400\n",
            "6942/6942 [==============================] - 3s 491us/step - loss: 0.0070 - val_loss: 0.0183\n",
            "Epoch 64/400\n",
            "6942/6942 [==============================] - 3s 495us/step - loss: 0.0070 - val_loss: 0.0198\n",
            "Epoch 65/400\n",
            "6942/6942 [==============================] - 3s 498us/step - loss: 0.0071 - val_loss: 0.0203\n",
            "Epoch 66/400\n",
            "6942/6942 [==============================] - 3s 503us/step - loss: 0.0071 - val_loss: 0.0178\n",
            "Epoch 67/400\n",
            "6942/6942 [==============================] - 3s 502us/step - loss: 0.0070 - val_loss: 0.0192\n",
            "Epoch 68/400\n",
            "6942/6942 [==============================] - 4s 511us/step - loss: 0.0070 - val_loss: 0.0189\n",
            "Epoch 69/400\n",
            "6942/6942 [==============================] - 4s 511us/step - loss: 0.0069 - val_loss: 0.0186\n",
            "Epoch 70/400\n",
            "6942/6942 [==============================] - 3s 500us/step - loss: 0.0068 - val_loss: 0.0183\n",
            "Epoch 71/400\n",
            "6942/6942 [==============================] - 3s 500us/step - loss: 0.0069 - val_loss: 0.0181\n",
            "Epoch 72/400\n",
            "6942/6942 [==============================] - 4s 511us/step - loss: 0.0070 - val_loss: 0.0190\n",
            "Epoch 73/400\n",
            "6942/6942 [==============================] - 4s 507us/step - loss: 0.0070 - val_loss: 0.0183\n",
            "Epoch 74/400\n",
            "6942/6942 [==============================] - 3s 499us/step - loss: 0.0070 - val_loss: 0.0192\n",
            "Epoch 75/400\n",
            "6942/6942 [==============================] - 3s 501us/step - loss: 0.0069 - val_loss: 0.0181\n",
            "Epoch 76/400\n",
            "6942/6942 [==============================] - 4s 507us/step - loss: 0.0069 - val_loss: 0.0177\n",
            "Epoch 77/400\n",
            "6942/6942 [==============================] - 4s 513us/step - loss: 0.0069 - val_loss: 0.0177\n",
            "Epoch 78/400\n",
            "6942/6942 [==============================] - 4s 513us/step - loss: 0.0068 - val_loss: 0.0189\n",
            "Epoch 79/400\n",
            "6942/6942 [==============================] - 3s 501us/step - loss: 0.0068 - val_loss: 0.0172\n",
            "Epoch 80/400\n",
            "6942/6942 [==============================] - 4s 507us/step - loss: 0.0068 - val_loss: 0.0180\n",
            "Epoch 81/400\n",
            "6942/6942 [==============================] - 3s 498us/step - loss: 0.0068 - val_loss: 0.0170\n",
            "Epoch 82/400\n",
            "6942/6942 [==============================] - 3s 498us/step - loss: 0.0067 - val_loss: 0.0176\n",
            "Epoch 83/400\n",
            "6942/6942 [==============================] - 4s 508us/step - loss: 0.0068 - val_loss: 0.0172\n",
            "Epoch 84/400\n",
            "6942/6942 [==============================] - 3s 495us/step - loss: 0.0068 - val_loss: 0.0167\n",
            "Epoch 85/400\n",
            "6942/6942 [==============================] - 3s 496us/step - loss: 0.0068 - val_loss: 0.0164\n",
            "Epoch 86/400\n",
            "6942/6942 [==============================] - 3s 492us/step - loss: 0.0068 - val_loss: 0.0176\n",
            "Epoch 87/400\n",
            "6942/6942 [==============================] - 3s 492us/step - loss: 0.0067 - val_loss: 0.0181\n",
            "Epoch 88/400\n",
            "6942/6942 [==============================] - 3s 491us/step - loss: 0.0067 - val_loss: 0.0167\n",
            "Epoch 89/400\n",
            "6942/6942 [==============================] - 3s 491us/step - loss: 0.0067 - val_loss: 0.0177\n",
            "Epoch 90/400\n",
            "6942/6942 [==============================] - 3s 494us/step - loss: 0.0068 - val_loss: 0.0185\n",
            "Epoch 91/400\n",
            "6942/6942 [==============================] - 3s 487us/step - loss: 0.0068 - val_loss: 0.0183\n",
            "Epoch 92/400\n",
            "6942/6942 [==============================] - 3s 493us/step - loss: 0.0068 - val_loss: 0.0177\n",
            "Epoch 93/400\n",
            "6942/6942 [==============================] - 3s 494us/step - loss: 0.0068 - val_loss: 0.0177\n",
            "Epoch 94/400\n",
            "6942/6942 [==============================] - 3s 493us/step - loss: 0.0067 - val_loss: 0.0170\n",
            "Epoch 95/400\n",
            "6942/6942 [==============================] - 3s 498us/step - loss: 0.0067 - val_loss: 0.0184\n",
            "Epoch 96/400\n",
            "6942/6942 [==============================] - 4s 508us/step - loss: 0.0069 - val_loss: 0.0173\n",
            "Epoch 97/400\n",
            "6942/6942 [==============================] - 4s 510us/step - loss: 0.0068 - val_loss: 0.0176\n",
            "Epoch 98/400\n",
            "6942/6942 [==============================] - 3s 494us/step - loss: 0.0068 - val_loss: 0.0179\n",
            "Epoch 99/400\n",
            "6942/6942 [==============================] - 3s 496us/step - loss: 0.0067 - val_loss: 0.0169\n",
            "Epoch 100/400\n",
            "6942/6942 [==============================] - 3s 498us/step - loss: 0.0066 - val_loss: 0.0170\n",
            "Epoch 101/400\n",
            "6942/6942 [==============================] - 3s 488us/step - loss: 0.0067 - val_loss: 0.0168\n",
            "Epoch 102/400\n",
            "6942/6942 [==============================] - 3s 487us/step - loss: 0.0067 - val_loss: 0.0163\n",
            "Epoch 103/400\n",
            "6942/6942 [==============================] - 3s 493us/step - loss: 0.0067 - val_loss: 0.0181\n",
            "Epoch 104/400\n",
            "6942/6942 [==============================] - 3s 490us/step - loss: 0.0067 - val_loss: 0.0171\n",
            "Epoch 105/400\n",
            "6942/6942 [==============================] - 3s 495us/step - loss: 0.0066 - val_loss: 0.0169\n",
            "Epoch 106/400\n",
            "6942/6942 [==============================] - 3s 490us/step - loss: 0.0066 - val_loss: 0.0179\n",
            "Epoch 107/400\n",
            "6942/6942 [==============================] - 3s 498us/step - loss: 0.0067 - val_loss: 0.0174\n",
            "Epoch 108/400\n",
            "6942/6942 [==============================] - 3s 491us/step - loss: 0.0067 - val_loss: 0.0172\n",
            "Epoch 109/400\n",
            "6942/6942 [==============================] - 3s 488us/step - loss: 0.0066 - val_loss: 0.0159\n",
            "Epoch 110/400\n",
            "6942/6942 [==============================] - 3s 494us/step - loss: 0.0066 - val_loss: 0.0154\n",
            "Epoch 111/400\n",
            "6942/6942 [==============================] - 3s 491us/step - loss: 0.0066 - val_loss: 0.0166\n",
            "Epoch 112/400\n",
            "6942/6942 [==============================] - 3s 487us/step - loss: 0.0066 - val_loss: 0.0167\n",
            "Epoch 113/400\n",
            "6942/6942 [==============================] - 3s 487us/step - loss: 0.0066 - val_loss: 0.0175\n",
            "Epoch 114/400\n",
            "6942/6942 [==============================] - 3s 493us/step - loss: 0.0066 - val_loss: 0.0166\n",
            "Epoch 115/400\n",
            "6942/6942 [==============================] - 3s 490us/step - loss: 0.0066 - val_loss: 0.0166\n",
            "Epoch 116/400\n",
            "6942/6942 [==============================] - 3s 492us/step - loss: 0.0066 - val_loss: 0.0163\n",
            "Epoch 117/400\n",
            "6942/6942 [==============================] - 3s 498us/step - loss: 0.0066 - val_loss: 0.0178\n",
            "Epoch 118/400\n",
            "6942/6942 [==============================] - 3s 491us/step - loss: 0.0065 - val_loss: 0.0163\n",
            "Epoch 119/400\n",
            "6942/6942 [==============================] - 3s 485us/step - loss: 0.0066 - val_loss: 0.0161\n",
            "Epoch 120/400\n",
            "6942/6942 [==============================] - 3s 492us/step - loss: 0.0065 - val_loss: 0.0150\n",
            "Epoch 121/400\n",
            "6942/6942 [==============================] - 3s 494us/step - loss: 0.0066 - val_loss: 0.0163\n",
            "Epoch 122/400\n",
            "6942/6942 [==============================] - 3s 492us/step - loss: 0.0065 - val_loss: 0.0163\n",
            "Epoch 123/400\n",
            "6942/6942 [==============================] - 3s 499us/step - loss: 0.0065 - val_loss: 0.0174\n",
            "Epoch 124/400\n",
            "6942/6942 [==============================] - 3s 503us/step - loss: 0.0066 - val_loss: 0.0165\n",
            "Epoch 125/400\n",
            "6942/6942 [==============================] - 4s 505us/step - loss: 0.0065 - val_loss: 0.0165\n",
            "Epoch 126/400\n",
            "6942/6942 [==============================] - 3s 504us/step - loss: 0.0067 - val_loss: 0.0164\n",
            "Epoch 127/400\n",
            "6942/6942 [==============================] - 3s 500us/step - loss: 0.0066 - val_loss: 0.0162\n",
            "Epoch 128/400\n",
            "6942/6942 [==============================] - 3s 495us/step - loss: 0.0065 - val_loss: 0.0161\n",
            "Epoch 129/400\n",
            "6942/6942 [==============================] - 3s 492us/step - loss: 0.0065 - val_loss: 0.0160\n",
            "Epoch 130/400\n",
            "6942/6942 [==============================] - 3s 480us/step - loss: 0.0065 - val_loss: 0.0154\n",
            "Epoch 131/400\n",
            "6942/6942 [==============================] - 3s 483us/step - loss: 0.0066 - val_loss: 0.0162\n",
            "Epoch 132/400\n",
            "6942/6942 [==============================] - 3s 493us/step - loss: 0.0065 - val_loss: 0.0164\n",
            "Epoch 133/400\n",
            "6942/6942 [==============================] - 4s 508us/step - loss: 0.0065 - val_loss: 0.0157\n",
            "Epoch 134/400\n",
            "6942/6942 [==============================] - 4s 506us/step - loss: 0.0065 - val_loss: 0.0154\n",
            "Epoch 135/400\n",
            "6942/6942 [==============================] - 4s 506us/step - loss: 0.0065 - val_loss: 0.0151\n",
            "Epoch 136/400\n",
            "6942/6942 [==============================] - 3s 491us/step - loss: 0.0065 - val_loss: 0.0171\n",
            "Epoch 137/400\n",
            "6942/6942 [==============================] - 3s 492us/step - loss: 0.0065 - val_loss: 0.0166\n",
            "Epoch 138/400\n",
            "6942/6942 [==============================] - 3s 494us/step - loss: 0.0065 - val_loss: 0.0161\n",
            "Epoch 139/400\n",
            "6942/6942 [==============================] - 3s 493us/step - loss: 0.0065 - val_loss: 0.0152\n",
            "Epoch 140/400\n",
            "6942/6942 [==============================] - 3s 493us/step - loss: 0.0064 - val_loss: 0.0166\n",
            "Epoch 141/400\n",
            "6942/6942 [==============================] - 3s 496us/step - loss: 0.0064 - val_loss: 0.0155\n",
            "Epoch 142/400\n",
            "6942/6942 [==============================] - 3s 495us/step - loss: 0.0065 - val_loss: 0.0156\n",
            "Epoch 143/400\n",
            "6942/6942 [==============================] - 3s 485us/step - loss: 0.0065 - val_loss: 0.0153\n",
            "Epoch 144/400\n",
            "6942/6942 [==============================] - 3s 492us/step - loss: 0.0065 - val_loss: 0.0153\n",
            "Epoch 145/400\n",
            "6942/6942 [==============================] - 4s 513us/step - loss: 0.0065 - val_loss: 0.0151\n",
            "Epoch 146/400\n",
            "6942/6942 [==============================] - 4s 520us/step - loss: 0.0065 - val_loss: 0.0153\n",
            "Epoch 147/400\n",
            "6942/6942 [==============================] - 4s 529us/step - loss: 0.0066 - val_loss: 0.0160\n",
            "Epoch 148/400\n",
            "6942/6942 [==============================] - 4s 523us/step - loss: 0.0066 - val_loss: 0.0156\n",
            "Epoch 149/400\n",
            "6942/6942 [==============================] - 4s 529us/step - loss: 0.0064 - val_loss: 0.0156\n",
            "Epoch 150/400\n",
            "6942/6942 [==============================] - 3s 490us/step - loss: 0.0065 - val_loss: 0.0153\n",
            "Epoch 151/400\n",
            "6942/6942 [==============================] - 3s 487us/step - loss: 0.0064 - val_loss: 0.0152\n",
            "Epoch 152/400\n",
            "6942/6942 [==============================] - 3s 484us/step - loss: 0.0064 - val_loss: 0.0161\n",
            "Epoch 153/400\n",
            "6942/6942 [==============================] - 3s 493us/step - loss: 0.0064 - val_loss: 0.0161\n",
            "Epoch 154/400\n",
            "6942/6942 [==============================] - 3s 487us/step - loss: 0.0065 - val_loss: 0.0157\n",
            "Epoch 155/400\n",
            "6942/6942 [==============================] - 3s 488us/step - loss: 0.0066 - val_loss: 0.0155\n",
            "Epoch 156/400\n",
            "6942/6942 [==============================] - 3s 485us/step - loss: 0.0066 - val_loss: 0.0172\n",
            "Epoch 157/400\n",
            "6942/6942 [==============================] - 3s 491us/step - loss: 0.0065 - val_loss: 0.0159\n",
            "Epoch 158/400\n",
            "6942/6942 [==============================] - 3s 484us/step - loss: 0.0065 - val_loss: 0.0145\n",
            "Epoch 159/400\n",
            "6942/6942 [==============================] - 3s 490us/step - loss: 0.0064 - val_loss: 0.0152\n",
            "Epoch 160/400\n",
            "6942/6942 [==============================] - 3s 489us/step - loss: 0.0064 - val_loss: 0.0155\n",
            "Epoch 161/400\n",
            "6942/6942 [==============================] - 3s 486us/step - loss: 0.0064 - val_loss: 0.0160\n",
            "Epoch 162/400\n",
            "6942/6942 [==============================] - 3s 491us/step - loss: 0.0064 - val_loss: 0.0154\n",
            "Epoch 163/400\n",
            "6942/6942 [==============================] - 3s 488us/step - loss: 0.0064 - val_loss: 0.0147\n",
            "Epoch 164/400\n",
            "6942/6942 [==============================] - 3s 483us/step - loss: 0.0064 - val_loss: 0.0157\n",
            "Epoch 165/400\n",
            "6942/6942 [==============================] - 3s 492us/step - loss: 0.0064 - val_loss: 0.0145\n",
            "Epoch 166/400\n",
            "6942/6942 [==============================] - 3s 495us/step - loss: 0.0065 - val_loss: 0.0154\n",
            "Epoch 167/400\n",
            "6942/6942 [==============================] - 3s 486us/step - loss: 0.0063 - val_loss: 0.0157\n",
            "Epoch 168/400\n",
            "6942/6942 [==============================] - 3s 494us/step - loss: 0.0065 - val_loss: 0.0150\n",
            "Epoch 169/400\n",
            "6942/6942 [==============================] - 3s 496us/step - loss: 0.0065 - val_loss: 0.0162\n",
            "Epoch 170/400\n",
            "6942/6942 [==============================] - 3s 502us/step - loss: 0.0065 - val_loss: 0.0154\n",
            "Epoch 171/400\n",
            "6942/6942 [==============================] - 4s 508us/step - loss: 0.0064 - val_loss: 0.0153\n",
            "Epoch 172/400\n",
            "6942/6942 [==============================] - 4s 504us/step - loss: 0.0063 - val_loss: 0.0162\n",
            "Epoch 173/400\n",
            "6942/6942 [==============================] - 3s 499us/step - loss: 0.0064 - val_loss: 0.0152\n",
            "Epoch 174/400\n",
            "6942/6942 [==============================] - 3s 491us/step - loss: 0.0063 - val_loss: 0.0148\n",
            "Epoch 175/400\n",
            "6942/6942 [==============================] - 3s 491us/step - loss: 0.0063 - val_loss: 0.0149\n",
            "Epoch 176/400\n",
            "6942/6942 [==============================] - 3s 490us/step - loss: 0.0064 - val_loss: 0.0160\n",
            "Epoch 177/400\n",
            "6942/6942 [==============================] - 3s 493us/step - loss: 0.0063 - val_loss: 0.0163\n",
            "Epoch 178/400\n",
            "6942/6942 [==============================] - 3s 493us/step - loss: 0.0064 - val_loss: 0.0147\n",
            "Epoch 179/400\n",
            "6942/6942 [==============================] - 3s 490us/step - loss: 0.0064 - val_loss: 0.0148\n",
            "Epoch 180/400\n",
            "6942/6942 [==============================] - 3s 485us/step - loss: 0.0064 - val_loss: 0.0171\n",
            "Epoch 181/400\n",
            "6942/6942 [==============================] - 3s 488us/step - loss: 0.0064 - val_loss: 0.0152\n",
            "Epoch 182/400\n",
            "6942/6942 [==============================] - 3s 490us/step - loss: 0.0063 - val_loss: 0.0157\n",
            "Epoch 183/400\n",
            "6942/6942 [==============================] - 3s 492us/step - loss: 0.0063 - val_loss: 0.0152\n",
            "Epoch 184/400\n",
            "6942/6942 [==============================] - 3s 498us/step - loss: 0.0063 - val_loss: 0.0159\n",
            "Epoch 185/400\n",
            "6942/6942 [==============================] - 3s 494us/step - loss: 0.0063 - val_loss: 0.0145\n",
            "Epoch 186/400\n",
            "6942/6942 [==============================] - 3s 471us/step - loss: 0.0063 - val_loss: 0.0153\n",
            "Epoch 187/400\n",
            "6942/6942 [==============================] - 3s 475us/step - loss: 0.0063 - val_loss: 0.0153\n",
            "Epoch 188/400\n",
            "6942/6942 [==============================] - 3s 466us/step - loss: 0.0063 - val_loss: 0.0157\n",
            "Epoch 189/400\n",
            "6942/6942 [==============================] - 3s 462us/step - loss: 0.0063 - val_loss: 0.0152\n",
            "Epoch 190/400\n",
            "6942/6942 [==============================] - 3s 459us/step - loss: 0.0063 - val_loss: 0.0146\n",
            "Epoch 191/400\n",
            "6942/6942 [==============================] - 3s 489us/step - loss: 0.0063 - val_loss: 0.0155\n",
            "Epoch 192/400\n",
            "6942/6942 [==============================] - 3s 480us/step - loss: 0.0064 - val_loss: 0.0161\n",
            "Epoch 193/400\n",
            "6942/6942 [==============================] - 3s 486us/step - loss: 0.0063 - val_loss: 0.0153\n",
            "Epoch 194/400\n",
            "6942/6942 [==============================] - 3s 486us/step - loss: 0.0063 - val_loss: 0.0147\n",
            "Epoch 195/400\n",
            "6942/6942 [==============================] - 3s 488us/step - loss: 0.0063 - val_loss: 0.0148\n",
            "Epoch 196/400\n",
            "6942/6942 [==============================] - 3s 490us/step - loss: 0.0063 - val_loss: 0.0149\n",
            "Epoch 197/400\n",
            "6942/6942 [==============================] - 3s 493us/step - loss: 0.0063 - val_loss: 0.0149\n",
            "Epoch 198/400\n",
            "6942/6942 [==============================] - 3s 491us/step - loss: 0.0063 - val_loss: 0.0152\n",
            "Epoch 199/400\n",
            "6942/6942 [==============================] - 3s 489us/step - loss: 0.0063 - val_loss: 0.0151\n",
            "Epoch 200/400\n",
            "6942/6942 [==============================] - 3s 491us/step - loss: 0.0063 - val_loss: 0.0141\n",
            "Epoch 201/400\n",
            "6942/6942 [==============================] - 3s 482us/step - loss: 0.0063 - val_loss: 0.0150\n",
            "Epoch 202/400\n",
            "6942/6942 [==============================] - 3s 488us/step - loss: 0.0064 - val_loss: 0.0164\n",
            "Epoch 203/400\n",
            "6942/6942 [==============================] - 3s 487us/step - loss: 0.0063 - val_loss: 0.0152\n",
            "Epoch 204/400\n",
            "6942/6942 [==============================] - 3s 489us/step - loss: 0.0063 - val_loss: 0.0148\n",
            "Epoch 205/400\n",
            "6942/6942 [==============================] - 3s 497us/step - loss: 0.0063 - val_loss: 0.0146\n",
            "Epoch 206/400\n",
            "6942/6942 [==============================] - 3s 489us/step - loss: 0.0063 - val_loss: 0.0162\n",
            "Epoch 207/400\n",
            "6942/6942 [==============================] - 3s 490us/step - loss: 0.0062 - val_loss: 0.0146\n",
            "Epoch 208/400\n",
            "6942/6942 [==============================] - 3s 488us/step - loss: 0.0062 - val_loss: 0.0157\n",
            "Epoch 209/400\n",
            "6942/6942 [==============================] - 3s 495us/step - loss: 0.0062 - val_loss: 0.0152\n",
            "Epoch 210/400\n",
            "6942/6942 [==============================] - 3s 488us/step - loss: 0.0062 - val_loss: 0.0148\n",
            "Epoch 211/400\n",
            "6942/6942 [==============================] - 3s 489us/step - loss: 0.0063 - val_loss: 0.0148\n",
            "Epoch 212/400\n",
            "6942/6942 [==============================] - 3s 492us/step - loss: 0.0062 - val_loss: 0.0141\n",
            "Epoch 213/400\n",
            "6942/6942 [==============================] - 3s 488us/step - loss: 0.0062 - val_loss: 0.0144\n",
            "Epoch 214/400\n",
            "6942/6942 [==============================] - 3s 490us/step - loss: 0.0063 - val_loss: 0.0144\n",
            "Epoch 215/400\n",
            "6942/6942 [==============================] - 3s 490us/step - loss: 0.0062 - val_loss: 0.0142\n",
            "Epoch 216/400\n",
            "6942/6942 [==============================] - 3s 489us/step - loss: 0.0062 - val_loss: 0.0150\n",
            "Epoch 217/400\n",
            "6942/6942 [==============================] - 3s 484us/step - loss: 0.0062 - val_loss: 0.0162\n",
            "Epoch 218/400\n",
            "6942/6942 [==============================] - 3s 488us/step - loss: 0.0062 - val_loss: 0.0147\n",
            "Epoch 219/400\n",
            "6942/6942 [==============================] - 3s 488us/step - loss: 0.0063 - val_loss: 0.0161\n",
            "Epoch 220/400\n",
            "6942/6942 [==============================] - 3s 486us/step - loss: 0.0063 - val_loss: 0.0151\n",
            "Epoch 221/400\n",
            "6942/6942 [==============================] - 3s 493us/step - loss: 0.0062 - val_loss: 0.0143\n",
            "Epoch 222/400\n",
            "6942/6942 [==============================] - 3s 491us/step - loss: 0.0063 - val_loss: 0.0145\n",
            "Epoch 223/400\n",
            "6942/6942 [==============================] - 3s 492us/step - loss: 0.0063 - val_loss: 0.0144\n",
            "Epoch 224/400\n",
            "6942/6942 [==============================] - 3s 490us/step - loss: 0.0062 - val_loss: 0.0144\n",
            "Epoch 225/400\n",
            "6942/6942 [==============================] - 3s 492us/step - loss: 0.0063 - val_loss: 0.0143\n",
            "Epoch 226/400\n",
            "6942/6942 [==============================] - 3s 486us/step - loss: 0.0062 - val_loss: 0.0148\n",
            "Epoch 227/400\n",
            "6942/6942 [==============================] - 3s 501us/step - loss: 0.0063 - val_loss: 0.0150\n",
            "Epoch 228/400\n",
            "6942/6942 [==============================] - 3s 494us/step - loss: 0.0063 - val_loss: 0.0148\n",
            "Epoch 229/400\n",
            "6942/6942 [==============================] - 3s 495us/step - loss: 0.0063 - val_loss: 0.0148\n",
            "Epoch 230/400\n",
            "6942/6942 [==============================] - 3s 492us/step - loss: 0.0063 - val_loss: 0.0157\n",
            "Epoch 231/400\n",
            "6942/6942 [==============================] - 3s 491us/step - loss: 0.0063 - val_loss: 0.0141\n",
            "Epoch 232/400\n",
            "6942/6942 [==============================] - 3s 490us/step - loss: 0.0063 - val_loss: 0.0150\n",
            "Epoch 233/400\n",
            "6942/6942 [==============================] - 3s 497us/step - loss: 0.0063 - val_loss: 0.0154\n",
            "Epoch 234/400\n",
            "6942/6942 [==============================] - 3s 495us/step - loss: 0.0063 - val_loss: 0.0166\n",
            "Epoch 235/400\n",
            "6942/6942 [==============================] - 3s 498us/step - loss: 0.0063 - val_loss: 0.0144\n",
            "Epoch 236/400\n",
            "6942/6942 [==============================] - 4s 521us/step - loss: 0.0062 - val_loss: 0.0149\n",
            "Epoch 237/400\n",
            "6942/6942 [==============================] - 4s 539us/step - loss: 0.0062 - val_loss: 0.0145\n",
            "Epoch 238/400\n",
            "6942/6942 [==============================] - 4s 539us/step - loss: 0.0062 - val_loss: 0.0150\n",
            "Epoch 239/400\n",
            "6942/6942 [==============================] - 4s 542us/step - loss: 0.0062 - val_loss: 0.0147\n",
            "Epoch 240/400\n",
            "6942/6942 [==============================] - 4s 542us/step - loss: 0.0062 - val_loss: 0.0145\n",
            "Epoch 241/400\n",
            "6942/6942 [==============================] - 3s 501us/step - loss: 0.0062 - val_loss: 0.0141\n",
            "Epoch 242/400\n",
            "6942/6942 [==============================] - 3s 500us/step - loss: 0.0061 - val_loss: 0.0142\n",
            "Epoch 243/400\n",
            "6942/6942 [==============================] - 3s 497us/step - loss: 0.0061 - val_loss: 0.0140\n",
            "Epoch 244/400\n",
            "6942/6942 [==============================] - 4s 510us/step - loss: 0.0061 - val_loss: 0.0142\n",
            "Epoch 245/400\n",
            "6942/6942 [==============================] - 4s 513us/step - loss: 0.0062 - val_loss: 0.0147\n",
            "Epoch 246/400\n",
            "6942/6942 [==============================] - 4s 513us/step - loss: 0.0062 - val_loss: 0.0154\n",
            "Epoch 247/400\n",
            "6942/6942 [==============================] - 4s 505us/step - loss: 0.0061 - val_loss: 0.0143\n",
            "Epoch 248/400\n",
            "6942/6942 [==============================] - 3s 500us/step - loss: 0.0061 - val_loss: 0.0153\n",
            "Epoch 249/400\n",
            "6942/6942 [==============================] - 3s 495us/step - loss: 0.0063 - val_loss: 0.0156\n",
            "Epoch 250/400\n",
            "6942/6942 [==============================] - 3s 499us/step - loss: 0.0063 - val_loss: 0.0149\n",
            "Epoch 251/400\n",
            "6942/6942 [==============================] - 3s 499us/step - loss: 0.0062 - val_loss: 0.0142\n",
            "Epoch 252/400\n",
            "6942/6942 [==============================] - 4s 507us/step - loss: 0.0062 - val_loss: 0.0141\n",
            "Epoch 253/400\n",
            "6942/6942 [==============================] - 4s 510us/step - loss: 0.0062 - val_loss: 0.0145\n",
            "Epoch 254/400\n",
            "6942/6942 [==============================] - 4s 509us/step - loss: 0.0062 - val_loss: 0.0144\n",
            "Epoch 255/400\n",
            "6942/6942 [==============================] - 4s 517us/step - loss: 0.0062 - val_loss: 0.0146\n",
            "Epoch 256/400\n",
            "6942/6942 [==============================] - 4s 513us/step - loss: 0.0061 - val_loss: 0.0149\n",
            "Epoch 257/400\n",
            "6942/6942 [==============================] - 4s 514us/step - loss: 0.0062 - val_loss: 0.0153\n",
            "Epoch 258/400\n",
            "6942/6942 [==============================] - 4s 519us/step - loss: 0.0062 - val_loss: 0.0139\n",
            "Epoch 259/400\n",
            "6942/6942 [==============================] - 4s 512us/step - loss: 0.0061 - val_loss: 0.0137\n",
            "Epoch 260/400\n",
            "6942/6942 [==============================] - 4s 508us/step - loss: 0.0061 - val_loss: 0.0144\n",
            "Epoch 261/400\n",
            "6942/6942 [==============================] - 3s 504us/step - loss: 0.0062 - val_loss: 0.0152\n",
            "Epoch 262/400\n",
            "6942/6942 [==============================] - 3s 495us/step - loss: 0.0062 - val_loss: 0.0141\n",
            "Epoch 263/400\n",
            "6942/6942 [==============================] - 3s 499us/step - loss: 0.0062 - val_loss: 0.0143\n",
            "Epoch 264/400\n",
            "6942/6942 [==============================] - 4s 513us/step - loss: 0.0062 - val_loss: 0.0135\n",
            "Epoch 265/400\n",
            "6942/6942 [==============================] - 4s 508us/step - loss: 0.0062 - val_loss: 0.0136\n",
            "Epoch 266/400\n",
            "6942/6942 [==============================] - 3s 502us/step - loss: 0.0062 - val_loss: 0.0148\n",
            "Epoch 267/400\n",
            "6942/6942 [==============================] - 3s 499us/step - loss: 0.0062 - val_loss: 0.0143\n",
            "Epoch 268/400\n",
            "6942/6942 [==============================] - 3s 492us/step - loss: 0.0061 - val_loss: 0.0150\n",
            "Epoch 269/400\n",
            "6942/6942 [==============================] - 4s 509us/step - loss: 0.0061 - val_loss: 0.0141\n",
            "Epoch 270/400\n",
            "6942/6942 [==============================] - 4s 511us/step - loss: 0.0061 - val_loss: 0.0140\n",
            "Epoch 271/400\n",
            "6942/6942 [==============================] - 3s 502us/step - loss: 0.0061 - val_loss: 0.0142\n",
            "Epoch 272/400\n",
            "6942/6942 [==============================] - 3s 502us/step - loss: 0.0062 - val_loss: 0.0143\n",
            "Epoch 273/400\n",
            "6942/6942 [==============================] - 4s 505us/step - loss: 0.0061 - val_loss: 0.0140\n",
            "Epoch 274/400\n",
            "6942/6942 [==============================] - 4s 508us/step - loss: 0.0061 - val_loss: 0.0141\n",
            "Epoch 275/400\n",
            "6942/6942 [==============================] - 3s 502us/step - loss: 0.0061 - val_loss: 0.0149\n",
            "Epoch 276/400\n",
            "6942/6942 [==============================] - 3s 489us/step - loss: 0.0061 - val_loss: 0.0143\n",
            "Epoch 277/400\n",
            "6942/6942 [==============================] - 3s 490us/step - loss: 0.0062 - val_loss: 0.0138\n",
            "Epoch 278/400\n",
            "6942/6942 [==============================] - 3s 496us/step - loss: 0.0062 - val_loss: 0.0141\n",
            "Epoch 279/400\n",
            "6942/6942 [==============================] - 3s 497us/step - loss: 0.0061 - val_loss: 0.0150\n",
            "Epoch 280/400\n",
            "6942/6942 [==============================] - 3s 496us/step - loss: 0.0061 - val_loss: 0.0141\n",
            "Epoch 281/400\n",
            "6942/6942 [==============================] - 3s 501us/step - loss: 0.0062 - val_loss: 0.0147\n",
            "Epoch 282/400\n",
            "6942/6942 [==============================] - 3s 502us/step - loss: 0.0061 - val_loss: 0.0147\n",
            "Epoch 283/400\n",
            "6942/6942 [==============================] - 4s 509us/step - loss: 0.0061 - val_loss: 0.0155\n",
            "Epoch 284/400\n",
            "6942/6942 [==============================] - 4s 508us/step - loss: 0.0063 - val_loss: 0.0151\n",
            "Epoch 285/400\n",
            "6942/6942 [==============================] - 4s 506us/step - loss: 0.0062 - val_loss: 0.0149\n",
            "Epoch 286/400\n",
            "6942/6942 [==============================] - 4s 508us/step - loss: 0.0061 - val_loss: 0.0140\n",
            "Epoch 287/400\n",
            "6942/6942 [==============================] - 3s 502us/step - loss: 0.0061 - val_loss: 0.0143\n",
            "Epoch 288/400\n",
            "6942/6942 [==============================] - 4s 513us/step - loss: 0.0061 - val_loss: 0.0139\n",
            "Epoch 289/400\n",
            "6942/6942 [==============================] - 3s 504us/step - loss: 0.0062 - val_loss: 0.0142\n",
            "Epoch 290/400\n",
            "6942/6942 [==============================] - 3s 498us/step - loss: 0.0061 - val_loss: 0.0142\n",
            "Epoch 291/400\n",
            "6942/6942 [==============================] - 3s 495us/step - loss: 0.0061 - val_loss: 0.0142\n",
            "Epoch 292/400\n",
            "6942/6942 [==============================] - 3s 504us/step - loss: 0.0062 - val_loss: 0.0137\n",
            "Epoch 293/400\n",
            "6942/6942 [==============================] - 4s 509us/step - loss: 0.0061 - val_loss: 0.0139\n",
            "Epoch 294/400\n",
            "6942/6942 [==============================] - 4s 519us/step - loss: 0.0062 - val_loss: 0.0140\n",
            "Epoch 295/400\n",
            "6942/6942 [==============================] - 4s 518us/step - loss: 0.0061 - val_loss: 0.0142\n",
            "Epoch 296/400\n",
            "6942/6942 [==============================] - 4s 512us/step - loss: 0.0061 - val_loss: 0.0138\n",
            "Epoch 297/400\n",
            "6942/6942 [==============================] - 3s 501us/step - loss: 0.0061 - val_loss: 0.0139\n",
            "Epoch 298/400\n",
            "6942/6942 [==============================] - 4s 507us/step - loss: 0.0061 - val_loss: 0.0148\n",
            "Epoch 299/400\n",
            "6942/6942 [==============================] - 4s 517us/step - loss: 0.0062 - val_loss: 0.0152\n",
            "Epoch 300/400\n",
            "6942/6942 [==============================] - 4s 510us/step - loss: 0.0061 - val_loss: 0.0151\n",
            "Epoch 301/400\n",
            "6942/6942 [==============================] - 3s 497us/step - loss: 0.0060 - val_loss: 0.0143\n",
            "Epoch 302/400\n",
            "6942/6942 [==============================] - 4s 504us/step - loss: 0.0061 - val_loss: 0.0142\n",
            "Epoch 303/400\n",
            "6942/6942 [==============================] - 3s 491us/step - loss: 0.0061 - val_loss: 0.0140\n",
            "Epoch 304/400\n",
            "6942/6942 [==============================] - 3s 499us/step - loss: 0.0060 - val_loss: 0.0134\n",
            "Epoch 305/400\n",
            "6942/6942 [==============================] - 4s 507us/step - loss: 0.0061 - val_loss: 0.0137\n",
            "Epoch 306/400\n",
            "6942/6942 [==============================] - 3s 499us/step - loss: 0.0060 - val_loss: 0.0136\n",
            "Epoch 307/400\n",
            "6942/6942 [==============================] - 4s 507us/step - loss: 0.0061 - val_loss: 0.0132\n",
            "Epoch 308/400\n",
            "6942/6942 [==============================] - 3s 500us/step - loss: 0.0062 - val_loss: 0.0140\n",
            "Epoch 309/400\n",
            "6942/6942 [==============================] - 3s 503us/step - loss: 0.0062 - val_loss: 0.0140\n",
            "Epoch 310/400\n",
            "6942/6942 [==============================] - 4s 505us/step - loss: 0.0061 - val_loss: 0.0137\n",
            "Epoch 311/400\n",
            "6942/6942 [==============================] - 3s 504us/step - loss: 0.0060 - val_loss: 0.0142\n",
            "Epoch 312/400\n",
            "6942/6942 [==============================] - 4s 507us/step - loss: 0.0061 - val_loss: 0.0143\n",
            "Epoch 313/400\n",
            "6942/6942 [==============================] - 4s 508us/step - loss: 0.0060 - val_loss: 0.0143\n",
            "Epoch 314/400\n",
            "6942/6942 [==============================] - 4s 510us/step - loss: 0.0061 - val_loss: 0.0142\n",
            "Epoch 315/400\n",
            "6942/6942 [==============================] - 3s 494us/step - loss: 0.0061 - val_loss: 0.0137\n",
            "Epoch 316/400\n",
            "6942/6942 [==============================] - 3s 494us/step - loss: 0.0060 - val_loss: 0.0142\n",
            "Epoch 317/400\n",
            "6942/6942 [==============================] - 3s 491us/step - loss: 0.0061 - val_loss: 0.0146\n",
            "Epoch 318/400\n",
            "6942/6942 [==============================] - 3s 496us/step - loss: 0.0061 - val_loss: 0.0140\n",
            "Epoch 319/400\n",
            "6942/6942 [==============================] - 3s 499us/step - loss: 0.0061 - val_loss: 0.0133\n",
            "Epoch 320/400\n",
            "6942/6942 [==============================] - 3s 496us/step - loss: 0.0060 - val_loss: 0.0138\n",
            "Epoch 321/400\n",
            "6942/6942 [==============================] - 3s 486us/step - loss: 0.0060 - val_loss: 0.0139\n",
            "Epoch 322/400\n",
            "6942/6942 [==============================] - 3s 486us/step - loss: 0.0061 - val_loss: 0.0143\n",
            "Epoch 323/400\n",
            "6942/6942 [==============================] - 3s 488us/step - loss: 0.0060 - val_loss: 0.0144\n",
            "Epoch 324/400\n",
            "6942/6942 [==============================] - 3s 494us/step - loss: 0.0061 - val_loss: 0.0137\n",
            "Epoch 325/400\n",
            "6942/6942 [==============================] - 4s 535us/step - loss: 0.0060 - val_loss: 0.0138\n",
            "Epoch 326/400\n",
            "6942/6942 [==============================] - 4s 538us/step - loss: 0.0061 - val_loss: 0.0142\n",
            "Epoch 327/400\n",
            "6942/6942 [==============================] - 4s 538us/step - loss: 0.0059 - val_loss: 0.0141\n",
            "Epoch 328/400\n",
            "6942/6942 [==============================] - 4s 537us/step - loss: 0.0060 - val_loss: 0.0140\n",
            "Epoch 329/400\n",
            "6942/6942 [==============================] - 4s 512us/step - loss: 0.0060 - val_loss: 0.0141\n",
            "Epoch 330/400\n",
            "6942/6942 [==============================] - 3s 486us/step - loss: 0.0060 - val_loss: 0.0141\n",
            "Epoch 331/400\n",
            "6942/6942 [==============================] - 3s 490us/step - loss: 0.0061 - val_loss: 0.0144\n",
            "Epoch 332/400\n",
            "6942/6942 [==============================] - 3s 488us/step - loss: 0.0061 - val_loss: 0.0136\n",
            "Epoch 333/400\n",
            "6942/6942 [==============================] - 3s 482us/step - loss: 0.0060 - val_loss: 0.0150\n",
            "Epoch 334/400\n",
            "6942/6942 [==============================] - 3s 491us/step - loss: 0.0061 - val_loss: 0.0137\n",
            "Epoch 335/400\n",
            "6942/6942 [==============================] - 3s 494us/step - loss: 0.0061 - val_loss: 0.0148\n",
            "Epoch 336/400\n",
            "6942/6942 [==============================] - 3s 491us/step - loss: 0.0061 - val_loss: 0.0142\n",
            "Epoch 337/400\n",
            "6942/6942 [==============================] - 3s 481us/step - loss: 0.0062 - val_loss: 0.0143\n",
            "Epoch 338/400\n",
            "6942/6942 [==============================] - 3s 484us/step - loss: 0.0061 - val_loss: 0.0145\n",
            "Epoch 339/400\n",
            "6942/6942 [==============================] - 3s 494us/step - loss: 0.0061 - val_loss: 0.0148\n",
            "Epoch 340/400\n",
            "6942/6942 [==============================] - 3s 492us/step - loss: 0.0060 - val_loss: 0.0137\n",
            "Epoch 341/400\n",
            "6942/6942 [==============================] - 3s 496us/step - loss: 0.0060 - val_loss: 0.0145\n",
            "Epoch 342/400\n",
            "6942/6942 [==============================] - 3s 493us/step - loss: 0.0060 - val_loss: 0.0141\n",
            "Epoch 343/400\n",
            "6942/6942 [==============================] - 3s 504us/step - loss: 0.0059 - val_loss: 0.0136\n",
            "Epoch 344/400\n",
            "6942/6942 [==============================] - 3s 497us/step - loss: 0.0060 - val_loss: 0.0134\n",
            "Epoch 345/400\n",
            "6942/6942 [==============================] - 3s 503us/step - loss: 0.0060 - val_loss: 0.0141\n",
            "Epoch 346/400\n",
            "6942/6942 [==============================] - 3s 503us/step - loss: 0.0060 - val_loss: 0.0140\n",
            "Epoch 347/400\n",
            "6942/6942 [==============================] - 4s 507us/step - loss: 0.0060 - val_loss: 0.0139\n",
            "Epoch 348/400\n",
            "6942/6942 [==============================] - 4s 513us/step - loss: 0.0059 - val_loss: 0.0144\n",
            "Epoch 349/400\n",
            "6942/6942 [==============================] - 4s 507us/step - loss: 0.0059 - val_loss: 0.0130\n",
            "Epoch 350/400\n",
            "6942/6942 [==============================] - 4s 505us/step - loss: 0.0060 - val_loss: 0.0139\n",
            "Epoch 351/400\n",
            "6942/6942 [==============================] - 3s 499us/step - loss: 0.0060 - val_loss: 0.0138\n",
            "Epoch 352/400\n",
            "6942/6942 [==============================] - 3s 500us/step - loss: 0.0060 - val_loss: 0.0137\n",
            "Epoch 353/400\n",
            "6942/6942 [==============================] - 3s 499us/step - loss: 0.0060 - val_loss: 0.0135\n",
            "Epoch 354/400\n",
            "6942/6942 [==============================] - 3s 498us/step - loss: 0.0060 - val_loss: 0.0134\n",
            "Epoch 355/400\n",
            "6942/6942 [==============================] - 3s 494us/step - loss: 0.0059 - val_loss: 0.0134\n",
            "Epoch 356/400\n",
            "6942/6942 [==============================] - 3s 491us/step - loss: 0.0059 - val_loss: 0.0131\n",
            "Epoch 357/400\n",
            "6942/6942 [==============================] - 3s 496us/step - loss: 0.0060 - val_loss: 0.0144\n",
            "Epoch 358/400\n",
            "6942/6942 [==============================] - 3s 496us/step - loss: 0.0060 - val_loss: 0.0136\n",
            "Epoch 359/400\n",
            "6942/6942 [==============================] - 3s 490us/step - loss: 0.0060 - val_loss: 0.0145\n",
            "Epoch 360/400\n",
            "6942/6942 [==============================] - 3s 500us/step - loss: 0.0059 - val_loss: 0.0147\n",
            "Epoch 361/400\n",
            "6942/6942 [==============================] - 3s 502us/step - loss: 0.0060 - val_loss: 0.0143\n",
            "Epoch 362/400\n",
            "6942/6942 [==============================] - 3s 501us/step - loss: 0.0059 - val_loss: 0.0134\n",
            "Epoch 363/400\n",
            "6942/6942 [==============================] - 3s 499us/step - loss: 0.0059 - val_loss: 0.0133\n",
            "Epoch 364/400\n",
            "6942/6942 [==============================] - 3s 497us/step - loss: 0.0059 - val_loss: 0.0140\n",
            "Epoch 365/400\n",
            "6942/6942 [==============================] - 3s 488us/step - loss: 0.0060 - val_loss: 0.0135\n",
            "Epoch 366/400\n",
            "6942/6942 [==============================] - 3s 488us/step - loss: 0.0060 - val_loss: 0.0138\n",
            "Epoch 367/400\n",
            "6942/6942 [==============================] - 3s 485us/step - loss: 0.0060 - val_loss: 0.0142\n",
            "Epoch 368/400\n",
            "6942/6942 [==============================] - 3s 488us/step - loss: 0.0059 - val_loss: 0.0137\n",
            "Epoch 369/400\n",
            "6942/6942 [==============================] - 3s 483us/step - loss: 0.0060 - val_loss: 0.0140\n",
            "Epoch 370/400\n",
            "6942/6942 [==============================] - 3s 482us/step - loss: 0.0060 - val_loss: 0.0140\n",
            "Epoch 371/400\n",
            "6942/6942 [==============================] - 3s 485us/step - loss: 0.0060 - val_loss: 0.0133\n",
            "Epoch 372/400\n",
            "6942/6942 [==============================] - 3s 481us/step - loss: 0.0059 - val_loss: 0.0134\n",
            "Epoch 373/400\n",
            "6942/6942 [==============================] - 3s 486us/step - loss: 0.0059 - val_loss: 0.0134\n",
            "Epoch 374/400\n",
            "6942/6942 [==============================] - 3s 485us/step - loss: 0.0059 - val_loss: 0.0139\n",
            "Epoch 375/400\n",
            "6942/6942 [==============================] - 3s 485us/step - loss: 0.0059 - val_loss: 0.0137\n",
            "Epoch 376/400\n",
            "6942/6942 [==============================] - 3s 488us/step - loss: 0.0060 - val_loss: 0.0148\n",
            "Epoch 377/400\n",
            "6942/6942 [==============================] - 3s 483us/step - loss: 0.0059 - val_loss: 0.0139\n",
            "Epoch 378/400\n",
            "6942/6942 [==============================] - 3s 482us/step - loss: 0.0059 - val_loss: 0.0144\n",
            "Epoch 379/400\n",
            "6942/6942 [==============================] - 3s 490us/step - loss: 0.0059 - val_loss: 0.0137\n",
            "Epoch 380/400\n",
            "6942/6942 [==============================] - 3s 487us/step - loss: 0.0060 - val_loss: 0.0145\n",
            "Epoch 381/400\n",
            "6942/6942 [==============================] - 3s 481us/step - loss: 0.0063 - val_loss: 0.0143\n",
            "Epoch 382/400\n",
            "6942/6942 [==============================] - 3s 490us/step - loss: 0.0061 - val_loss: 0.0134\n",
            "Epoch 383/400\n",
            "6942/6942 [==============================] - 3s 490us/step - loss: 0.0061 - val_loss: 0.0150\n",
            "Epoch 384/400\n",
            "6942/6942 [==============================] - 3s 480us/step - loss: 0.0061 - val_loss: 0.0151\n",
            "Epoch 385/400\n",
            "6942/6942 [==============================] - 3s 472us/step - loss: 0.0060 - val_loss: 0.0133\n",
            "Epoch 386/400\n",
            "6942/6942 [==============================] - 3s 471us/step - loss: 0.0060 - val_loss: 0.0135\n",
            "Epoch 387/400\n",
            "6942/6942 [==============================] - 3s 477us/step - loss: 0.0060 - val_loss: 0.0144\n",
            "Epoch 388/400\n",
            "6942/6942 [==============================] - 3s 479us/step - loss: 0.0060 - val_loss: 0.0140\n",
            "Epoch 389/400\n",
            "6942/6942 [==============================] - 3s 477us/step - loss: 0.0059 - val_loss: 0.0137\n",
            "Epoch 390/400\n",
            "6942/6942 [==============================] - 3s 477us/step - loss: 0.0060 - val_loss: 0.0135\n",
            "Epoch 391/400\n",
            "6942/6942 [==============================] - 3s 483us/step - loss: 0.0060 - val_loss: 0.0136\n",
            "Epoch 392/400\n",
            "6942/6942 [==============================] - 3s 478us/step - loss: 0.0060 - val_loss: 0.0147\n",
            "Epoch 393/400\n",
            "6942/6942 [==============================] - 3s 478us/step - loss: 0.0060 - val_loss: 0.0151\n",
            "Epoch 394/400\n",
            "6942/6942 [==============================] - 3s 476us/step - loss: 0.0059 - val_loss: 0.0140\n",
            "Epoch 395/400\n",
            "6942/6942 [==============================] - 3s 475us/step - loss: 0.0059 - val_loss: 0.0140\n",
            "Epoch 396/400\n",
            "6942/6942 [==============================] - 3s 476us/step - loss: 0.0060 - val_loss: 0.0140\n",
            "Epoch 397/400\n",
            "6942/6942 [==============================] - 3s 478us/step - loss: 0.0059 - val_loss: 0.0140\n",
            "Epoch 398/400\n",
            "6942/6942 [==============================] - 3s 478us/step - loss: 0.0059 - val_loss: 0.0141\n",
            "Epoch 399/400\n",
            "6942/6942 [==============================] - 3s 478us/step - loss: 0.0059 - val_loss: 0.0131\n",
            "Epoch 400/400\n",
            "6942/6942 [==============================] - 3s 481us/step - loss: 0.0059 - val_loss: 0.0140\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "HjRiSGzFfA-G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        },
        "outputId": "7ae301fe-d036-4949-e5a7-3ba070f9f857"
      },
      "cell_type": "code",
      "source": [
        "# Sampling examples passed through autoencoder\n",
        "prediction = autoencoder.predict(X_test)\n",
        "sample_images(prediction, y_test)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAd8AAAEeCAYAAADLtB9JAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f520fa9e208>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "wKRSGKWOvPNJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        },
        "outputId": "47afc8f8-b8d6-40f9-d201-b756e5af25d8"
      },
      "cell_type": "code",
      "source": [
        "# Same examples but their original form for comparison\n",
        "sample_images(X_test, y_test)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAd8AAAEeCAYAAADLtB9JAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nA9vbRng7UoKNAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f520b72d710>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "-Wg-PSmp1-Du",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Saving model to be loaded on GAN later\n",
        "\n",
        "encoder_json = encoder.to_json()\n",
        "decoder_json = decoder.to_json()\n",
        "ae_json = autoencoder.to_json()\n",
        "\n",
        "with open(\"encoder.json\", \"w\") as json_file:\n",
        "    json_file.write(encoder_json)\n",
        "with open(\"decoder.json\", \"w\") as json_file:\n",
        "    json_file.write(decoder_json)\n",
        "with open(\"autoencoder.json\", \"w\") as json_file:\n",
        "    json_file.write(ae_json)\n",
        "\n",
        "# Saving weights to disk\n",
        "\n",
        "encoder.save_weights('encoder.h5')\n",
        "decoder.save_weights('decoder.h5')\n",
        "autoencoder.save_weights('autoencoder.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kkx_c_8G9QiG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}